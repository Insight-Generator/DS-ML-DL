should think about how to get feedback so we can retrain our models
  * Rate per visualization (Think about set of visualizations too?)
  * User created visualizations (We failed and they manually picked x axis, y axis, and set of filters)

* Using https://nlp.stanford.edu/projects/glove/ , Raymond was able to use it to import word embeddings to find similarity between one word and a set of words. Split per word (ex: gender vs race/ethnicity split to [gender vs race, gender vs ethnicity])
  * In the future, we can combine the two ways for better weight and correlation. 
  ** At the start will use hard coded weights but if it works may switch to bayesian methods
  * Some entries in a database may be shorthanded such as auth_id. Possibly switch datasets to account for that.
  * Need to brainstorm methodology of training a model that will determine if the type of insight generated is incorrect. Ex: scatter plot generated but heatmap would have been better
* Matt updated files
* We should think about how to get feedback so we can retrain our models
  * Rate per visualization (Think about set of visualizations too?)
  * User created visualizations (We failed and they manually picked x axis, y axis, and set of filters)