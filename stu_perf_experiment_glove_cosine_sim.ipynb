{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Dataset\nhttps://www.kaggle.com/spscientist/students-performance-in-exams\n\nModified from\nhttps://github.com/adsieg/text_similarity/blob/529337838839515b5b2e8cb6cac3452d3ff663dc/.ipynb_checkpoints/Different%20Embeddings%20%2B%20Cosine%20Similarity%20%2B%20HeatMap%20illustration-checkpoint.ipynb"},{"metadata":{},"cell_type":"markdown","source":"# Loading GloVe model \nWikipedia 2014 + Gigaword 5 (6B tokens 50d)\nhttps://nlp.stanford.edu/projects/glove/\nhttps://www.kaggle.com/rtatman/glove-global-vectors-for-word-representation\n\nNOTE: Consider switching datasets because of shortening of words in db"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# NOTE1: GloVe embeddings look like this:\n# the 0.418 0.24968 -0.41242 0.1217 0.34527 -0.044457 -0.49688 -0.17862 -0.00066023 -0.6566 ...\n# NOTE2: Has embeddings for punctuation\n# NOTE3: Does not have any uppercase words in embeddings\n\nimport numpy as np\n\nglove_model = {}\nwith open('../input/glove-global-vectors-for-word-representation/glove.6B.200d.txt', encoding=\"utf8\" ) as f:\n    section = f.readlines()\n    \nfor line in section:\n    split_line = line.split(\" \")\n    word = split_line[0]\n    if word.isalpha(): # Will be used to only keep the words in embeddings\n        embedding = np.array([float(val) for val in split_line[1:]]) # 1: for only the embeddings. Reference Note1\n        glove_model[word] = embedding","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Cosine Similarity"},{"metadata":{"trusted":true},"cell_type":"code","source":"import scipy\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport re\nfrom nltk.corpus import stopwords\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess(raw_text):\n    # keep only words\n    letters_only_text = re.sub(\"[^a-zA-Z]\", \" \", raw_text)\n    \n    # convert to lower case and split \n    words = letters_only_text.lower().split()\n\n    # remove stopwords\n    stopword_set = set(stopwords.words(\"english\"))\n    cleaned_words = list(set([w for w in words if w not in stopword_set]))\n    return cleaned_words\n\ndef cosine_distance_between_two_words(word1, word2):\n    return (1- scipy.spatial.distance.cosine(glove_model[word1], glove_model[word2]))\n\ndef calculate_heat_matrix_for_two_sentences(s1,s2):\n    s1 = preprocess(s1)\n    s2 = preprocess(s2)\n    result_list = [[cosine_distance_between_two_words(word1, word2) for word2 in s2] for word1 in s1]\n    result_df = pd.DataFrame(result_list)\n    result_df.columns = s2\n    result_df.index = s1\n    return result_df\n\ndef heat_map_matrix_between_two_sentences(s1,s2):\n    df = calculate_heat_matrix_for_two_sentences(s1,s2)\n    fig, ax = plt.subplots(figsize=(5,5)) \n    ax_blue = sns.heatmap(df, cmap=\"YlGnBu\", annot=True)\n    return ax_blue","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ss1 = 'test preparation course'\nss2 = 'writing score'\n\n# gender\n# race/ethnicity\n# parental level of education\n# lunch\n# test preparation course\n# math score\n# reading score\n# writing score\n\n# model = glove_model\nheat_map_matrix_between_two_sentences(ss1,ss2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}